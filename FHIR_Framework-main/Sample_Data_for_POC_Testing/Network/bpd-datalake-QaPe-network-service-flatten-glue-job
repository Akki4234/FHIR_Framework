import sys

from pyspark import SparkContext
from pyspark.sql import functions as F
from pyspark.sql import types as pt
from pyspark.sql.window import Window as w
from awsglue.context import GlueContext
from datetime import datetime, timedelta
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import explode, explode_outer, col, when, split, element_at, to_date, lit, desc, dense_rank, \
    concat_ws
from pyspark.sql.types import *
from pyspark.sql import DataFrame
from pyspark.sql import Row
from functools import reduce
from pyspark.sql.utils import AnalysisException
import boto3

try:
    args = getResolvedOptions(sys.argv, ['last_date'])
    last_date = args['last_date']

except:
    print('arguments are missing')
    last_date = None

try:
    args = getResolvedOptions(sys.argv, ['s3_input_path','s3_output_path', 'metadata_path', 'environment_path','JOB_NAME', 'environment', 'sns_arn'])
    environment_path = args['environment_path']
    s3_input_path = args['s3_input_path']
    job_name = args['JOB_NAME']
    environment = args['environment']
    sns_arn = args['sns_arn']
    job_run_id = args['JOB_RUN_ID']

    sns_client = boto3.client('sns')

    def unionall(*dfs):
        return reduce(DataFrame.union, dfs)

    def has_column(df, col):
        try:
            df[col]
            return True
        except AnalysisException:
            return False

    def col_check(df, col_name: str, col_rename: str):
        if has_column(df, col_name):
            final_df = df.withColumn(col_rename, col(col_name))
        else:
            final_df = df.withColumn(col_rename, lit(None).cast("string"))

        return final_df 
    
    def get_last_date(spark):
        try:
            network_data = args["metadata_path"]
            metadata_df = spark.read.option("header", True).csv(network_data)
            metadata_df = metadata_df.where(metadata_df.job_name_met == 'network')
            last_date = metadata_df.select('last_date').agg({'last_date': 'max'}).collect()[0][0]
        except Exception as e:
            msg = 'Error reading metadata file or Metadata file has no data, check the metadata file'
            raise Exception(msg).with_traceback(e.__traceback__)
        return last_date

    def union_all(*dfs):
        return reduce(DataFrame.union, dfs)


    def current_run_date(current_pdate):
        try:
            sys_date=datetime.today()
            print('System Date-',sys_date)
            if current_pdate >= sys_date:
                raise Exception(f'The current_pdate -{current_pdate} value is greater than or equal to system date hence Exiting....' )
        except Exception as e: 
                raise e

    def process_data(spark, last_date):
        """[summary]

        :param spark: [description]
        :type spark: [type]
        """
        if last_date is not None:
            last_date = last_date
            print(f'last_date from workflow trigger-- {last_date}')
        else:
            last_date = get_last_date(spark)
        schema = StructType([StructField('id', StringType(), True), StructField('identifier', ArrayType(StructType([StructField('assigner', StructType([StructField('display', StringType(), True)]), True), StructField('value', StringType(), True)]), True), True), StructField('meta', StructType([StructField('profile', ArrayType(StringType(), True), True), StructField('versionId', StringType(), True)]), True), StructField('name', StringType(), True), StructField('partOf', StructType([StructField('reference', StringType(), True)]), True), StructField('resourceType', StringType(), True), StructField('text', StructType([StructField('div', StringType(), True), StructField('status', StringType(), True)]), True)])

        if isinstance(last_date, str):
            last_pdate = datetime.strptime(last_date, "%Y-%m-%d")
            current_pdate = last_pdate + timedelta(1)
            current_run_date(current_pdate)
            year= current_pdate.strftime("%Y")
            month = current_pdate.strftime("%m")
            day= current_pdate.strftime("%d")
            file_path = f"{s3_input_path}year={year}/month={month}/day={day}"
            try:
                print(f"Reading data from partition - {file_path}")
                cols=["year","month","day"]
                network_df = spark.read.schema(schema).json(file_path, multiLine=True) \
                                .withColumn('year', F.lit(year)).withColumn('month', F.lit(month)).withColumn('day', F.lit(day))
                network_df = network_df.withColumn("pdate",concat_ws("-",*cols).cast("date"))
                if len(network_df.head(1)) > 0:
                    data_avail_flag = True
                else:
                    print(f'path - {file_path} exists but no data')
                    data_avail_flag = False
            except:
                data_avail_flag = False
                print(f"File path - {file_path} doesn't exist.")

        
        if data_avail_flag:     
            network_df = network_df.withColumn("versionId", network_df.meta.versionId) \
                                                .drop(network_df.meta) \
                                                .withColumn("netw_identifier", explode_outer(network_df.identifier)) \
                                                .withColumn("NETWORK_CD", col("netw_identifier.value")) \
                                                .drop("identifier") \
                                                .drop("text") \
                                                .drop(col("netw_identifier")) \
                                                .drop("partOf") \
                                                .withColumn("NETWORK_NM", network_df.name) \
                                                .drop('name')                                             
                                        
            network_df = network_df.distinct()
        
            col_list = network_df.columns 
            for each_col in col_list:
                if each_col in ['id', 'versionId','year', 'month', 'day']:
                    pass
                else:
                    network_df = network_df.withColumn('flag', F.when(F.col(each_col).isNull(),F.lit(0)).otherwise(F.lit(1)))
                    network_df = network_df.withColumn(each_col , F.last(each_col,True).over(w.partitionBy('id','versionId','year', 'month', 'day').orderBy(F.col('flag').desc()).rowsBetween(-sys.maxsize,0))).drop('flag')  
                    network_df = network_df.distinct()

            df_output = network_df.distinct()

            print('***************Storing data in temp location ***************')
            output_bucket = args["s3_output_path"]
            df_output = df_output.withColumn('month', F.when(F.length(F.col('month')) == 1, F.concat(F.lit('0'), F.col('month'))).otherwise(F.col('month'))) \
                                .withColumn('day', F.when(F.length(F.col('day')) == 1, F.concat(F.lit('0'), F.col('day'))).otherwise(F.col('day')))

            df_output = df_output.drop('pdate')
            df_output.write.mode('overwrite').format('parquet').partitionBy('year', 'month', 'day').option("partitionOverwriteMode", "dynamic").save(output_bucket)
            print('Network Data loaded')
        else:
            print('Network data not available at source - {}'.format(datetime.now()))
        return current_pdate

    if __name__ == "__main__":
        run_date = datetime.today()
        current_pdate = None
        glueContext = GlueContext(SparkContext.getOrCreate())
        spark = glueContext.spark_session
        current_pdate = process_data(spark, last_date)
        current_pdate = datetime.strptime((current_pdate).strftime('%Y-%m-%d'),"%Y-%m-%d").date()
        # Prepare metadata table
        print("Initiating processing for metadata table")
        processing_time = datetime.now()
        job_name_met = 'network'
        print(f"last+p_date {current_pdate} ----------- processing_time {processing_time} ---------")
        meta_data_path = args["metadata_path"]
        meta_data_df = spark.createDataFrame([(current_pdate, processing_time, job_name_met)],
                                    ['last_date', 'run_date', 'job_name_met'])
        meta_data_df.coalesce(1).write.mode('append').format('csv').option('header', 'true').save(meta_data_path)
        print("Processed metadata table")


except Exception as e:
    # preparing data for SNS notification
    status = 'FINISHED_FAILURE'
    if current_pdate: 
        message = (f' Job_Name: {job_name}\n Status: {status} \n environment: {environment} \n Job_Run_ID: {job_run_id} \n Data_Date: {current_pdate} \
        \n Run_Date: {run_date} \n ERROR: {e}')
    else: 
        message = (f' Job_Name: {job_name}\n Status: {status} \n environment: {environment} \n Job_Run_ID: {job_run_id} \n Run_Date: {run_date} \n ERROR: {e}')
    subject = (f'[{environment}] {job_name} {status}')
    sns_client.publish(TopicArn=sns_arn,
        Message=message,
        Subject=subject)
    raise e

                                         
                                             